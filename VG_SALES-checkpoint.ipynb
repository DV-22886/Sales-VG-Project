{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae3900-5821-4338-a954-55394570f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "df.info()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e933d-da9b-4b5b-8574-bd860c6daeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8b407-bfa5-46b8-817b-256aeff003db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts)\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "print(missing_percent)\n",
    "\n",
    "#df.nunique().sort_values()                  # how many unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba5a7f-6ebf-49ca-bf2d-b2cb4e11ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "print(\"Categorical columns:\", list(cat_cols))\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67964eaa-2ac2-4aec-8b8d-613ad0eb6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df['Year'] = pd.to_numeric(df['Year'], errors='coerce')  # ensure numeric\n",
    "#df['Year'].fillna(df['Year'].median(), inplace=True)\n",
    "# Option 1: Fill with 'Unknown'\n",
    "#df['Publisher'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Option 2: Drop rows with missing Publisher\n",
    "# df.dropna(subset=['Publisher'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d5e5f-8471-4d60-a456-0ff17d5b3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numeric Year values with median\n",
    "df['Year'] = df['Year'].fillna(df['Year'].median())\n",
    "\n",
    "# Fill missing categorical Publisher values with 'Unknown'\n",
    "df['Publisher'] = df['Publisher'].fillna('Unknown')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60876610-eddd-4976-a1e6-d8281701a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes #confirm numeric vs categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee054aac-e35c-448e-b9b4-7d69e2cfdfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- 1️⃣ Overview ---\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n",
    "print(\"\\nUnique values per column:\\n\", df.nunique())\n",
    "\n",
    "# --- 2️⃣ Categorical Columns Analysis ---\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "print(\"\\nCategorical Columns:\", list(cat_cols))\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].value_counts().head(10))  # top 10 categories\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().index[:10])\n",
    "    plt.title(f\"Top 10 {col} Values\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 3️⃣ Numerical Columns Analysis ---\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nNumerical Columns:\", list(num_cols))\n",
    "display(df[num_cols].describe())\n",
    "\n",
    "# Histograms\n",
    "df[num_cols].hist(bins=30, figsize=(12,8))\n",
    "plt.suptitle(\"Numerical Columns Distributions\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplots\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df[num_cols])\n",
    "plt.title(\"Numerical Columns Boxplots\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4️⃣ Bivariate Analysis ---\n",
    "# Total Global Sales by Genre\n",
    "genre_sales = df.groupby('Genre')['Global_Sales'].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=genre_sales.values, y=genre_sales.index)\n",
    "plt.title(\"Total Global Sales by Genre\")\n",
    "plt.xlabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# Average Global Sales by Platform (Top 10)\n",
    "platform_sales = df.groupby('Platform')['Global_Sales'].mean().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=platform_sales.values, y=platform_sales.index)\n",
    "plt.title(\"Average Global Sales by Platform (Top 10)\")\n",
    "plt.xlabel(\"Average Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# Total Global Sales by Publisher (Top 10)\n",
    "publisher_sales = df.groupby('Publisher')['Global_Sales'].sum().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=publisher_sales.values, y=publisher_sales.index)\n",
    "plt.title(\"Total Global Sales by Publisher (Top 10)\")\n",
    "plt.xlabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 5️⃣ Time Trend Analysis ---\n",
    "# Global Sales Over Years\n",
    "yearly_sales = df.groupby('Year')['Global_Sales'].sum().reset_index()\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.lineplot(data=yearly_sales, x='Year', y='Global_Sales', marker='o')\n",
    "plt.title(\"Global Sales Trend Over Years\")\n",
    "plt.ylabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# Genre-wise Sales Over Years\n",
    "genre_year = df.groupby(['Year','Genre'])['Global_Sales'].sum().unstack().fillna(0)\n",
    "genre_year.plot(figsize=(12,6))\n",
    "plt.title(\"Global Sales by Genre Over Years\")\n",
    "plt.ylabel(\"Global Sales (millions)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c15bf-9aee-4271-a345-36c95598830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1️⃣ Identify Blockbuster Games ---\n",
    "# Define blockbuster as Global Sales > 5 million\n",
    "df['Is_Blockbuster'] = df['Global_Sales'] > 5\n",
    "blockbusters = df[df['Is_Blockbuster']]\n",
    "print(\"Number of Blockbuster Games:\", len(blockbusters))\n",
    "print(\"\\nTop 10 Blockbusters:\")\n",
    "display(blockbusters[['Name','Platform','Genre','Publisher','Global_Sales']].sort_values(by='Global_Sales', ascending=False).head(10))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='Global_Sales', y='Name', data=blockbusters.sort_values(by='Global_Sales', ascending=False).head(10))\n",
    "plt.title(\"Top 10 Blockbuster Games\")\n",
    "plt.xlabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 2️⃣ Top Publishers by Global Sales ---\n",
    "publisher_sales_total = df.groupby('Publisher')['Global_Sales'].sum().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=publisher_sales_total.values, y=publisher_sales_total.index)\n",
    "plt.title(\"Top 10 Publishers by Global Sales\")\n",
    "plt.xlabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3️⃣ Top Publishers Per Decade ---\n",
    "# Create Decade column\n",
    "df['Decade'] = (df['Year']//10)*10\n",
    "\n",
    "decade_publisher = df.groupby(['Decade','Publisher'])['Global_Sales'].sum().reset_index()\n",
    "top_publishers_per_decade = decade_publisher.sort_values(['Decade','Global_Sales'], ascending=[True,False]).groupby('Decade').head(3)\n",
    "print(\"\\nTop 3 Publishers Per Decade:\")\n",
    "display(top_publishers_per_decade)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='Decade', y='Global_Sales', hue='Publisher', data=top_publishers_per_decade)\n",
    "plt.title(\"Top 3 Publishers per Decade\")\n",
    "plt.ylabel(\"Global Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4️⃣ Regional Insights ---\n",
    "# Total Sales by Region\n",
    "region_cols = ['NA_Sales','EU_Sales','JP_Sales','Other_Sales']\n",
    "region_sales = df[region_cols].sum().sort_values(ascending=False)\n",
    "print(\"\\nTotal Sales by Region:\")\n",
    "print(region_sales)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=region_sales.index, y=region_sales.values)\n",
    "plt.title(\"Total Sales by Region\")\n",
    "plt.ylabel(\"Sales (millions)\")\n",
    "plt.show()\n",
    "\n",
    "# Top Genres per Region\n",
    "for region in region_cols:\n",
    "    top_genres = df.groupby('Genre')[region].sum().sort_values(ascending=False).head(5)\n",
    "    print(f\"\\nTop 5 Genres in {region}:\")\n",
    "    print(top_genres)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(x=top_genres.values, y=top_genres.index)\n",
    "    plt.title(f\"Top 5 Genres in {region}\")\n",
    "    plt.xlabel(\"Sales (millions)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b671a1-5f28-434f-85ce-839d433031ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check duplicates\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "\n",
    "# Check for invalid Year values (e.g., 0 or future years)\n",
    "print(\"Invalid Years:\", df[(df['Year'] <= 0) | (df['Year'] > 2025)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d6f6f-6ef1-4759-a7e7-c213b9a4ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df.to_csv(r\"C:\\Users\\Siddhartha\\Documents\\vgsales_cleaned.csv\", index=False)\n",
    "print(\"Cleaned dataset exported successfully as CSV!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33981b8-5226-42f7-a7f6-257c5c4dd22d",
   "metadata": {},
   "source": [
    "Summary of Key Insights from This EDA Block\n",
    "\n",
    "Genres: Sports and Action dominate global sales.\n",
    "\n",
    "Platforms: Certain consoles dominate specific periods (PS2, Wii, X360).\n",
    "\n",
    "Publishers: Nintendo, EA, Activision are top revenue contributors.\n",
    "\n",
    "Sales Distribution: Most games sell moderately, few blockbusters drive most revenue.\n",
    "\n",
    "Time Trends: Peak sales occurred in mid-2000s; genre popularity shifts with console generations.\n",
    "\n",
    "Correlations: Regional sales correlate with global sales — North America is the largest market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56628d84-99ea-4e05-ad9e-ce0dcc078894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- Step 0: Import Libraries -------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "# Replace with your CSV file path\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "\n",
    "# Preview dataset\n",
    "print(\"First 5 rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Categorical Columns -------------------\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "print(\"\\nCategorical Columns Detected:\", cat_cols)\n",
    "\n",
    "# ------------------- Step 3: Fill Missing Values with Mode -------------------\n",
    "for col in cat_cols:\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_val)  # Avoids pandas inplace warning\n",
    "\n",
    "# ------------------- Step 4: Analyze Categorical Columns -------------------\n",
    "print(\"\\n=== Categorical Column Analysis ===\")\n",
    "for col in cat_cols:\n",
    "    cardinality = df[col].nunique()\n",
    "    print(f\"\\n--- Column: {col} ---\")\n",
    "    print(\"Unique Categories:\", df[col].unique())\n",
    "    print(\"Frequency / Count:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"Percentage (%):\")\n",
    "    print((df[col].value_counts(normalize=True)*100).round(2))\n",
    "    print(\"Mode:\", df[col].mode()[0])\n",
    "    print(\"Cardinality:\", cardinality)\n",
    "\n",
    "# ------------------- Step 5: Visualize Categorical Columns -------------------\n",
    "max_pie_categories = 10  # skip pie chart if categories > 10\n",
    "\n",
    "for col in cat_cols:\n",
    "    cardinality = df[col].nunique()\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    # Bar chart with no warnings\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.countplot(x=col, data=df, palette='Set2', hue=None, edgecolor='black')\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Pie chart (only if unique categories <= max_pie_categories)\n",
    "    plt.subplot(1,2,2)\n",
    "    if cardinality <= max_pie_categories:\n",
    "        df[col].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('Set2'))\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.ylabel('')\n",
    "    else:\n",
    "        plt.text(0.5,0.5,f\"Pie chart skipped\\n({cardinality} categories)\",\n",
    "                 horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✅ Categorical analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b80b02-ff17-4569-8ad2-55d255d91100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "# Replace with your CSV file path\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Preview dataset\n",
    "print(\"First 5 rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Numerical Columns -------------------\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nNumerical Columns Detected:\", num_cols)\n",
    "\n",
    "# ------------------- Step 3: Basic Statistics -------------------\n",
    "print(\"\\n=== Basic Statistics for Numerical Columns ===\")\n",
    "print(df[num_cols].describe())  # count, mean, std, min, quartiles, max\n",
    "\n",
    "# ------------------- Step 4: Visualizations -------------------\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    # Histogram + KDE\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(df[col], kde=True, bins=20, color='skyblue')\n",
    "    plt.title(f'Histogram + KDE of {col}')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(x=df[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------- Step 5: Correlation Heatmap -------------------\n",
    "if len(num_cols) > 1:  # only if more than 1 numerical column\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap of Numerical Columns')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✅ Numerical analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4185ee4-874f-45d8-a058-b320ef719da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- Step 0: Imports -------------------\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "print(\"First 5 rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Columns -------------------\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nCategorical Columns:\", cat_cols)\n",
    "print(\"Numerical Columns:\", num_cols)\n",
    "\n",
    "# ------------------- Step 3: Create Sample for Fast Plots -------------------\n",
    "df_sample = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "\n",
    "# =================== UNIVARIATE ANALYSIS ===================\n",
    "print(\"\\n=== Univariate Analysis ===\")\n",
    "\n",
    "# --- Categorical Columns ---\n",
    "for col in cat_cols:\n",
    "    if df[col].nunique() <= 10:  # skip high-cardinality\n",
    "        print(f\"\\n--- Column: {col} ---\")\n",
    "        print(df[col].value_counts())\n",
    "        print((df[col].value_counts(normalize=True)*100).round(2))\n",
    "        \n",
    "        # Bar chart\n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.countplot(x=col, data=df_sample, color='skyblue', edgecolor='red')\n",
    "        plt.title(f'Bar Chart of {col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# --- Numerical Columns ---\n",
    "for col in num_cols:\n",
    "    print(f\"\\n--- Column: {col} ---\")\n",
    "    print(df[col].describe())\n",
    "    \n",
    "    # Histogram + KDE\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(df_sample[col], kde=True, bins=20, color='skyblue')\n",
    "    plt.title(f'Histogram + KDE of {col}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(6,2))\n",
    "    sns.boxplot(x=df_sample[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# =================== BIVARIATE ANALYSIS ===================\n",
    "print(\"\\n=== Bivariate Analysis ===\")\n",
    "\n",
    "# --- Numerical vs Numerical (skip pairplot for large datasets) ---\n",
    "if len(num_cols) > 1 and len(df) <= 2000:\n",
    "    sns.pairplot(df_sample[num_cols])\n",
    "    plt.suptitle('Pairplot of Numerical Columns', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Numerical Columns')\n",
    "plt.show()\n",
    "\n",
    "# --- Categorical vs Categorical (limited to ≤10 categories) ---\n",
    "for col1, col2 in combinations(cat_cols, 2):\n",
    "    if df[col1].nunique() <= 10 and df[col2].nunique() <= 10:\n",
    "        ctab = pd.crosstab(df[col1], df[col2])\n",
    "        print(f\"\\nCross-tabulation: {col1} vs {col2}\")\n",
    "        print(ctab)\n",
    "        \n",
    "        # Stacked bar chart\n",
    "        ctab.plot(kind='bar', stacked=True, figsize=(8,3), colormap='Set2')\n",
    "        plt.title(f'Stacked Bar: {col1} vs {col2}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# --- Numerical vs Categorical (limited to ≤10 categories) ---\n",
    "for cat in cat_cols:\n",
    "    if df[cat].nunique() <= 10:\n",
    "        for num in num_cols:\n",
    "            plt.figure(figsize=(6,3))\n",
    "            sns.boxplot(x=cat, y=num, data=df_sample, palette='Set2')\n",
    "            plt.title(f'Boxplot of {num} grouped by {cat}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\n✅ Fast Univariate and Bivariate analysis complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badd014-99aa-402b-a651-60bfd914f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------- Step 0: Imports -------------------\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "print(\"First 5 rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Check Data Types -------------------\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Detect categorical and numerical columns\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nCategorical Columns:\", cat_cols)\n",
    "print(\"Numerical Columns:\", num_cols)\n",
    "\n",
    "# ------------------- Step 3: Handle Categorical Columns -------------------\n",
    "for col in cat_cols:\n",
    "    missing = df[col].isnull().sum()\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "    print(f\"\\n{col} -> Missing: {missing}, Mode: {mode_val}, Unique: {df[col].nunique()}\")\n",
    "\n",
    "# ------------------- Step 4: Handle Numerical Columns -------------------\n",
    "print(\"\\n=== Numerical Columns Description ===\")\n",
    "print(df[num_cols].describe())\n",
    "\n",
    "# ------------------- Step 5: Univariate Analysis (Fast Version) -------------------\n",
    "# Use a sample for plotting\n",
    "df_sample = df.sample(n=min(1000, len(df)), random_state=42)\n",
    "\n",
    "# Categorical\n",
    "for col in cat_cols:\n",
    "    if df[col].nunique() <= 10:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.countplot(x=col, data=df_sample, color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Bar Chart of {col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# Numerical\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(df_sample[col], kde=True, bins=20, color='skyblue')\n",
    "    plt.title(f'Histogram + KDE of {col}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6,2))\n",
    "    sns.boxplot(x=df_sample[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------- Step 6: Bivariate Analysis -------------------\n",
    "# Numerical vs Numerical (skip pairplot for large datasets)\n",
    "if len(num_cols) > 1 and df.shape[0] <= 2000:\n",
    "    sns.pairplot(df_sample[num_cols])\n",
    "    plt.suptitle('Pairplot of Numerical Columns', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Numerical Columns')\n",
    "plt.show()\n",
    "\n",
    "# Categorical vs Categorical\n",
    "for col1, col2 in combinations(cat_cols, 2):\n",
    "    if df[col1].nunique() <= 10 and df[col2].nunique() <= 10:\n",
    "        ctab = pd.crosstab(df[col1], df[col2])\n",
    "        ctab.plot(kind='bar', stacked=True, figsize=(8,4), colormap='Set2')\n",
    "        plt.title(f'Stacked Bar: {col1} vs {col2}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# Numerical vs Categorical\n",
    "for cat in cat_cols:\n",
    "    for num in num_cols:\n",
    "        if df[cat].nunique() <= 10:\n",
    "            plt.figure(figsize=(6,3))\n",
    "            sns.boxplot(x=cat, y=num, data=df_sample, palette='Set2')\n",
    "            plt.title(f'Boxplot of {num} grouped by {cat}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "# ------------------- Step 7: Outlier Detection -------------------\n",
    "print(\"\\n=== Outlier Detection (IQR Method) ===\")\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "    print(f\"{col}: {outliers.shape[0]} outliers detected\")\n",
    "\n",
    "# ------------------- Step 8: Export Cleaned Dataset -------------------\n",
    "df.to_csv('vgsales_cleaned_fast.csv', index=False)\n",
    "print(\"\\n✅ Cleaned dataset exported as 'vgsales_cleaned_fast.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc52943-d7e7-42ee-91de-035f1986b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------- Step 3: Outlier Detection & Handling -------------------\n",
    "# Choose method: 'remove' or 'cap'\n",
    "method = 'cap'  # change to 'remove' if you want to delete outlier rows\n",
    "\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "    print(f\"{col}: {outliers.shape[0]} outliers detected\")\n",
    "    \n",
    "    # Visualize before handling\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col} (Before Handling)')\n",
    "    plt.show()\n",
    "    \n",
    "    if method == 'remove':\n",
    "        df = df[~((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR))]\n",
    "        print(f\"{col}: Outliers removed, new shape: {df.shape}\")\n",
    "    \n",
    "    elif method == 'cap':\n",
    "        df[col] = df[col].apply(lambda x: Q3 + 1.5*IQR if x > Q3 + 1.5*IQR else x)\n",
    "        df[col] = df[col].apply(lambda x: Q1 - 1.5*IQR if x < Q1 - 1.5*IQR else x)\n",
    "        print(f\"{col}: Outliers capped\")\n",
    "    \n",
    "    # Visualize after handling\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col} (After Handling)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e968c-19b5-439b-a160-f4b65ebf8c4b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "df = pd.read_csv(\"path_to_your_dataset.csv\")\n",
    "\n",
    "# ------------------- Step 1: Identify Target and Features -------------------\n",
    "target_col = 'TargetColumn'  # <-- Replace with your target column name\n",
    "y = df[target_col]           # Target\n",
    "X = df.drop(columns=[target_col])  # Features\n",
    "\n",
    "# ------------------- Step 2: Encode Categorical Variables -------------------\n",
    "# Encode categorical features in X using One-Hot Encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Encode target if categorical (binary example)\n",
    "if y.dtype == 'object':\n",
    "    if y.nunique() == 2:\n",
    "        y = y.map({y.unique()[0]: 0, y.unique()[1]: 1})\n",
    "    else:\n",
    "        y = pd.get_dummies(y, drop_first=True)\n",
    "\n",
    "# ------------------- Check Results -------------------\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"\\nSample of X:\")\n",
    "print(X.head())\n",
    "print(\"\\nSample of y:\")\n",
    "print(y.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eaeea-3046-454a-99ef-42dffce6fe7b",
   "metadata": {},
   "source": [
    "# ------------------- Step 0: Imports -------------------\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "df = pd.read_csv(\"C:/Users/Siddhartha/vgsales.csv\")  # Replace with your path\n",
    "print(\"First 5 rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Identify Features and Target -------------------\n",
    "target_col = 'Global_Sales'  # Column we want to predict\n",
    "y = df[target_col]           # Target\n",
    "X = df.drop(columns=[target_col])  # Features\n",
    "\n",
    "# ------------------- Step 3: Feature Engineering / Encoding -------------------\n",
    "# Handle categorical variables using One-Hot Encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Handle missing values if any (numeric fill with median example)\n",
    "for col in X.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# ------------------- Step 4: Train-Test Split -------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------- Step 5: Model Training -------------------\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------- Step 6: Model Evaluation -------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics for regression\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(\"RMSE:\", round(rmse, 2))\n",
    "print(\"R² Score:\", round(r2, 2))\n",
    "\n",
    "# ------------------- Step 7: Make Predictions -------------------\n",
    "# Example: Predict on new data\n",
    "# new_data = pd.DataFrame({...})  # create new dataset with same features\n",
    "# predictions = model.predict(new_data)\n",
    "\n",
    "# ------------------- Step 8: Export Cleaned / Prepared Dataset -------------------\n",
    "X.to_csv(\"vg_sales_cleaned_for_PowerBI.csv\", index=False)\n",
    "print(\"\\n✅ Cleaned dataset exported for Power BI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3504a7-abe5-429a-80c3-58b4d01d95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Step 0: Imports -------------------\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ------------------- Step 1: Load Dataset (Sample for Speed) -------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\").sample(n=1000, random_state=42)\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Columns -------------------\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nCategorical Columns:\", cat_cols)\n",
    "print(\"Numerical Columns:\", num_cols)\n",
    "\n",
    "# ------------------- Step 3: Handle Missing Values -------------------\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# ------------------- Step 4: Univariate Analysis (Tables Only) -------------------\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col} Value Counts:\\n\", df[col].value_counts())\n",
    "for col in num_cols:\n",
    "    print(f\"\\n{col} Description:\\n\", df[col].describe())\n",
    "\n",
    "# ------------------- Step 5: Bivariate Analysis (Tables Only) -------------------\n",
    "# Numerical vs Numerical: correlation\n",
    "print(\"\\nCorrelation Matrix:\\n\", df[num_cols].corr())\n",
    "\n",
    "# Categorical vs Categorical (small cardinality only)\n",
    "for col1, col2 in combinations(cat_cols, 2):\n",
    "    if df[col1].nunique() <= 3 and df[col2].nunique() <= 3:\n",
    "        print(f\"\\nCross-tab: {col1} vs {col2}\\n\", pd.crosstab(df[col1], df[col2]))\n",
    "\n",
    "# Numerical vs Categorical (small cardinality)\n",
    "for cat in cat_cols:\n",
    "    for num in num_cols:\n",
    "        if df[cat].nunique() <= 3:\n",
    "            print(f\"\\n{num} grouped by {cat}:\\n\", df.groupby(cat)[num].describe())\n",
    "\n",
    "# ------------------- Step 6: Outlier Handling (Cap Method) -------------------\n",
    "method = 'cap'\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    if method == 'cap':\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "# ------------------- Step 7: ML Workflow -------------------\n",
    "target_col = 'Global_Sales'\n",
    "y = df[target_col]\n",
    "\n",
    "# Drop target + all columns contributing to it to avoid leakage\n",
    "leak_cols = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "X = df.drop(columns=leak_cols)\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model Training\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(\"RMSE:\", round(rmse, 2))\n",
    "print(\"R² Score:\", round(r2, 2))\n",
    "\n",
    "# ------------------- Step 8: Export Cleaned Dataset -------------------\n",
    "X.to_csv(\"vg_sales_cleaned_no_leakage.csv\", index=False)\n",
    "print(\"\\n✅ Cleaned dataset exported for Power BI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021820c-26b0-4d57-9f32-3f35437339b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Step 0: Imports -------------------\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Columns -------------------\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nCategorical Columns:\", cat_cols)\n",
    "print(\"Numerical Columns:\", num_cols)\n",
    "\n",
    "# ------------------- Step 3: Handle Missing Values -------------------\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# ------------------- Step 4: Univariate Analysis (Tables Only) -------------------\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col} Value Counts:\\n\", df[col].value_counts())\n",
    "for col in num_cols:\n",
    "    print(f\"\\n{col} Description:\\n\", df[col].describe())\n",
    "\n",
    "# ------------------- Step 5: Bivariate Analysis (Tables Only) -------------------\n",
    "# Numerical vs Numerical: correlation\n",
    "print(\"\\nCorrelation Matrix:\\n\", df[num_cols].corr())\n",
    "\n",
    "# Categorical vs Categorical (small cardinality only)\n",
    "for col1, col2 in combinations(cat_cols, 2):\n",
    "    if df[col1].nunique() <= 3 and df[col2].nunique() <= 3:\n",
    "        print(f\"\\nCross-tab: {col1} vs {col2}\\n\", pd.crosstab(df[col1], df[col2]))\n",
    "\n",
    "# Numerical vs Categorical (small cardinality)\n",
    "for cat in cat_cols:\n",
    "    for num in num_cols:\n",
    "        if df[cat].nunique() <= 3:\n",
    "            print(f\"\\n{num} grouped by {cat}:\\n\", df.groupby(cat)[num].describe())\n",
    "\n",
    "# ------------------- Step 6: Outlier Handling (Cap Method) -------------------\n",
    "method = 'cap'\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    if method == 'cap':\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "# ------------------- Step 7: ML Workflow -------------------\n",
    "target_col = 'Global_Sales'\n",
    "y = df[target_col]\n",
    "\n",
    "# Drop target + all columns contributing to it to avoid leakage\n",
    "leak_cols = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "X = df.drop(columns=leak_cols)\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model Training\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(\"RMSE:\", round(rmse, 2))\n",
    "print(\"R² Score:\", round(r2, 2))\n",
    "\n",
    "# ------------------- Step 8: Export Cleaned Dataset -------------------\n",
    "X.to_csv(\"vg_sales_cleaned_no_leakage.csv\", index=False)\n",
    "print(\"\\n✅ Cleaned dataset exported for Power BI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a46ea2-16ed-40ed-93b6-e055bda2d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Step 0: Imports -------------------\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ------------------- Step 1: Load Dataset -------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# ------------------- Step 2: Detect Columns -------------------\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# ------------------- Step 3: Handle Missing Values -------------------\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# ------------------- Step 4: Univariate Analysis (Fast) -------------------\n",
    "# Print top 5 value counts for categorical columns\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col} top categories:\\n\", df[col].value_counts().head())\n",
    "\n",
    "# Print descriptive stats for numerical columns\n",
    "print(\"\\nNumerical columns description:\\n\", df[num_cols].describe())\n",
    "\n",
    "# ------------------- Step 5: Bivariate Analysis (Fast) -------------------\n",
    "# Correlation matrix for numeric\n",
    "print(\"\\nCorrelation matrix (numeric columns):\\n\", df[num_cols].corr())\n",
    "\n",
    "# ------------------- Step 6: Outlier Handling -------------------\n",
    "method = 'cap'\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "# ------------------- Step 7: ML Workflow -------------------\n",
    "target_col = 'Global_Sales'\n",
    "y = df[target_col]\n",
    "\n",
    "# Drop columns contributing to the target to avoid leakage\n",
    "leak_cols = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "X = df.drop(columns=leak_cols)\n",
    "\n",
    "# Encode categorical columns (one-hot)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model training\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"RMSE:\", round(rmse, 2))\n",
    "print(\"R²:\", round(r2, 2))\n",
    "\n",
    "# ------------------- Step 8: Export Cleaned Dataset -------------------\n",
    "X.to_csv(\"vg_sales_cleaned_fast.csv\", index=False)\n",
    "print(\"\\n✅ Dataset exported for Power BI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11673e09-07d6-4317-988c-201b7218c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Siddhartha\\vgsales.csv\")\n",
    "\n",
    "# Select only low-cardinality categorical columns for ML\n",
    "safe_cat_cols = ['Platform', 'Genre']  # only these categorical features\n",
    "num_cols = ['Year', 'Rank']  # example numeric features (excluding sales columns)\n",
    "\n",
    "# Handle missing values\n",
    "for col in safe_cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Prepare target and features\n",
    "target_col = 'Global_Sales'\n",
    "y = df[target_col]\n",
    "\n",
    "# Drop columns contributing to target\n",
    "leak_cols = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "X = df.drop(columns=leak_cols)\n",
    "\n",
    "# Keep only safe features\n",
    "X = X[safe_cat_cols + num_cols]\n",
    "\n",
    "# One-Hot Encode\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", round(rmse, 2))\n",
    "print(\"R²:\", round(r2, 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
